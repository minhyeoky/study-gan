{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install tensorflow==2.0.0-beta0 \n",
    "!conda install -y -q seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n",
      "0.9.0\n",
      "1.17.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print(sns.__version__)\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "keras = tf.keras\n",
    "\n",
    "K = keras.backend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 2\n",
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "<PrefetchDataset shapes: (None, 28, 28, 1), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = dataset[0][0], dataset[1][0]\n",
    "Y_train, Y_test = dataset[0][1], dataset[1][1]\n",
    "\n",
    "del(dataset)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[0], cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train.reshape([*X_train.shape] + [1]).astype('float32')\n",
    "X_test = X_test.reshape([*X_test.shape] + [1]).astype('float32')\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train = (X_train - 127.5) / 127.5\n",
    "X_test = (X_test - 127.5) / 127.5\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "\n",
    "data = data.shuffle(batch_size * 10).batch(batch_size).prefetch(batch_size * 5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECONVOLUTION MODEL (GENERATOR)\n",
    "\n",
    "- INPUT : a random noise with dimension of (100,)\n",
    "- OUTPUT : generated image with dimension of (28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    layers_kwargs = {\n",
    "        'kernel_initializer': keras.initializers.TruncatedNormal(seed=0),\n",
    "        'activation': 'relu',\n",
    "\n",
    "    }\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=256*7*7, input_shape=(100,), **layers_kwargs))\n",
    "    model.add(keras.layers.Reshape(target_shape=[7, 7, 256]))\n",
    "    \n",
    "    layers_kwargs['padding'] = 'same'\n",
    "    \n",
    "    model.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
    "                                          strides=(1, 1), **layers_kwargs))\n",
    "    model.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
    "                                          strides=(1, 1), **layers_kwargs))\n",
    "    model.add(keras.layers.Conv2DTranspose(filters=32, kernel_size=(5, 5),\n",
    "                                          strides=(2, 2), **layers_kwargs))\n",
    "\n",
    "    layers_kwargs['activation'] = 'tanh'\n",
    "    \n",
    "    model.add(keras.layers.Conv2DTranspose(filters=1, kernel_size=(5, 5),\n",
    "                                          strides=(2, 2), **layers_kwargs))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR MODEL\n",
    "\n",
    "- INPUT : a image with 28x28x1 size.\n",
    "- OUTPUT : the probability whether the image is fake or real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    layers_kwargs = {\n",
    "        'kernel_size': (5, 5),\n",
    "        'strides': (2, 2),\n",
    "        'padding': 'same',\n",
    "        'activation': 'relu',\n",
    "        'kernel_initializer': keras.initializers.TruncatedNormal()\n",
    "    }\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=64, input_shape=(28, 28, 1), **layers_kwargs))\n",
    "    model.add(keras.layers.Conv2D(filters=256, **layers_kwargs))\n",
    "#     model.add(keras.layers.Conv2D(filters=))\n",
    "    model.add(keras.layers.Reshape(target_shape=[7*7*256]))\n",
    "    model.add(keras.layers.Dense(units=1, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "\n",
    "\n",
    "$$\\min_{\\theta_d}\\max_{\\theta_g} E_{x\\sim p_{data}}\\log D(x) + E_{z\\sim p_{z}}\\log(1 - D(G(z)))$$\n",
    "\n",
    "$$p_z \\sim 표준정규분포$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(y_pred):\n",
    "    y_true = keras.backend.ones_like(y_pred)\n",
    "    cross_entropy = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return tf.math.reduce_mean(cross_entropy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(y_pred, x):\n",
    "    if x:\n",
    "        y_true = keras.backend.ones_like(y_pred)\n",
    "    else:\n",
    "        y_true = keras.backend.zeros_like(y_pred)\n",
    "    \n",
    "    cross_entropy = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return tf.math.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=326488, shape=(), dtype=float32, numpy=5.1416497>"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_loss([[1.0],[1.0],[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g = keras.optimizers.Adam(learning_rate=0.001)\n",
    "opt_d = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_45 (Conv2DT (None, 7, 7, 128)         819328    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_46 (Conv2DT (None, 7, 7, 64)          204864    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_47 (Conv2DT (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_48 (Conv2DT (None, 28, 28, 1)         801       \n",
      "=================================================================\n",
      "Total params: 2,343,169\n",
      "Trainable params: 2,343,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 256)         409856    \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 12545     \n",
      "=================================================================\n",
      "Total params: 424,065\n",
      "Trainable params: 424,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensors(tensors):\n",
    "    for each in tensors:\n",
    "        print(f'{each.name} {each.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(real_data):\n",
    "    z = np.random.randn(batch_size, noise_dim)\n",
    "    \n",
    "    with tf.GradientTape() as tape_g, tf.GradientTape() as tape_d:\n",
    "        Gz = generator(inputs=z)\n",
    "        Dz = discriminator(inputs=Gz)\n",
    "        Dx = discriminator(inputs=real_data)\n",
    "        \n",
    "        loss_g = generator_loss(Dz)\n",
    "        loss_d = discriminator_loss(Dz, x=False) * 1/2\n",
    "        loss_d += discriminator_loss(Dx, x=True) * 1/2\n",
    "        \n",
    "        print(f'losses: {loss_g}, {loss_d}')\n",
    "#         print(loss_g.shape, loss_d.shape)\n",
    "        \n",
    "    grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n",
    "    grads_d = tape_d.gradient(loss_d, discriminator.trainable_variables)\n",
    "    opt_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n",
    "    opt_d.apply_gradients(zip(grads_d, discriminator.trainable_variables))\n",
    "    \n",
    "    return Gz\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor):\n",
    "    img = tf.reshape(tensor, shape=[28, 28])\n",
    "    plt.imshow(img.numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses: 2.6784653663635254, 0.038024719804525375\n",
      "-------------------- 0 --------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses: 3.9227864742279053, 0.014363092370331287\n",
      "losses: 5.446538925170898, 0.008662188425660133\n",
      "losses: 6.742048263549805, 0.015382545068860054\n",
      "losses: 7.543512344360352, 0.018319973722100258\n",
      "losses: 7.6621551513671875, 0.008313266560435295\n",
      "losses: 7.4473724365234375, 0.005188156850636005\n",
      "losses: 6.998743534088135, 0.002675164956599474\n",
      "losses: 6.477642059326172, 0.0014522997662425041\n",
      "losses: 5.979926586151123, 0.0015809221658855677\n",
      "losses: 5.5923871994018555, 0.0022264630533754826\n",
      "losses: 5.374017715454102, 0.0025651270989328623\n",
      "losses: 5.380867958068848, 0.0024759117513895035\n",
      "losses: 5.608699798583984, 0.0019213727209717035\n",
      "losses: 6.011850357055664, 0.0013148174621164799\n",
      "losses: 6.516355991363525, 0.0009101213072426617\n",
      "losses: 7.040152549743652, 0.0007987527642399073\n",
      "losses: 7.537771701812744, 0.0005920855328440666\n",
      "losses: 7.983597278594971, 0.0009440594003535807\n",
      "losses: 8.33657455444336, 0.00029356410959735513\n",
      "losses: 8.649137496948242, 0.002712058136239648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-429-a5d39159ccbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'-------------------- {idx} --------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-423-5ae658c9d466>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(real_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         print(loss_g.shape, loss_d.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgrads_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mgrads_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mopt_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1188\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for idx, real_data in enumerate(data):\n",
    "        gz = train_step(real_data)\n",
    "        if idx  == 0:\n",
    "            print(f'-------------------- {idx} --------------------')\n",
    "            imshow(real_data[0])\n",
    "            imshow(gz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
